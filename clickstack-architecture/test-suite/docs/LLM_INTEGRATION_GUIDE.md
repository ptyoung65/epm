# LLM Integration Guide - AIRIS-MON AIOps System\n\n## 개요\n\nAIRIS-MON 시스템에 통합된 포괄적인 LLM(Large Language Model) 시스템으로, Ollama Gemma 3.1B를 주요 로컬 모델로 사용하고 외부 LLM 제공자들과의 통합을 지원합니다.\n\n## 🚀 주요 특징\n\n### 1. Multi-LLM 지원\n- **Primary**: Ollama Gemma 3.1B (로컬)\n- **Fallback**: Google Gemini Pro, OpenAI GPT-4, Anthropic Claude\n- **지능형 라우팅**: 작업 유형에 따른 최적 모델 자동 선택\n- **로드 밸런싱**: 성능 기반 동적 로드 분산\n\n### 2. 한국어 특화 최적화\n- **전처리**: 한국어 텍스트 정규화 및 최적화\n- **후처리**: 응답 품질 향상 및 일관성 보장\n- **품질 검증**: 한국어 품질 자동 평가\n- **문맥 인식**: 한국 문화 및 관습 고려\n\n### 3. 실시간 스트리밍\n- **Server-Sent Events**: 실시간 응답 스트리밍\n- **Progressive Loading**: 점진적 응답 로딩\n- **에러 핸들링**: 견고한 스트림 에러 처리\n\n## 🏗️ 시스템 아키텍처\n\n```\n┌─────────────────┐    ┌──────────────────┐    ┌─────────────────┐\n│   Web Client    │◄──►│  Multi-LLM       │◄──►│  LLM Config     │\n│                 │    │  Router          │    │  Manager        │\n└─────────────────┘    └──────────────────┘    └─────────────────┘\n                                │                        │\n                                ▼                        ▼\n                    ┌──────────────────┐    ┌─────────────────┐\n                    │ Korean Language  │    │ Health Monitor  │\n                    │ Processor        │    │ & Metrics       │\n                    └──────────────────┘    └─────────────────┘\n                                │\n                                ▼\n        ┌─────────────┬─────────┴─────────┬─────────────┬─────────────┐\n        │   Ollama    │     Gemini       │   OpenAI    │   Claude    │\n        │ Gemma 3.1B  │      Pro         │    GPT-4    │   Sonnet    │\n        │   (Local)   │    (Cloud)       │  (Cloud)    │  (Cloud)    │\n        └─────────────┴──────────────────┴─────────────┴─────────────┘\n```\n\n## 📋 API 엔드포인트\n\n### 1. 채팅 완성 API\n```http\nPOST /api/llm/chat\nContent-Type: application/json\n\n{\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"안녕하세요. AIRIS-MON 시스템에 대해 설명해주세요.\"\n    }\n  ],\n  \"taskType\": \"korean-chat\",\n  \"providerId\": \"ollama\",\n  \"options\": {\n    \"temperature\": 0.7,\n    \"formality\": \"formal\"\n  },\n  \"stream\": false\n}\n```\n\n**응답:**\n```json\n{\n  \"success\": true,\n  \"data\": {\n    \"message\": {\n      \"role\": \"assistant\",\n      \"content\": \"AIRIS-MON은 포괄적인 AIOps 모니터링 시스템입니다...\"\n    },\n    \"usage\": {\n      \"prompt_tokens\": 25,\n      \"completion_tokens\": 150,\n      \"total_tokens\": 175\n    }\n  },\n  \"taskType\": \"korean-chat\",\n  \"provider\": \"ollama\"\n}\n```\n\n### 2. 데이터 분석 API\n```http\nPOST /api/llm/analyze\nContent-Type: application/json\n\n{\n  \"data\": {\n    \"cpu_usage\": [75, 82, 90, 95, 88],\n    \"memory_usage\": [60, 65, 70, 75, 72],\n    \"response_time\": [120, 150, 200, 300, 180]\n  },\n  \"analysisType\": \"performance\",\n  \"language\": \"korean\",\n  \"providerId\": \"ollama\"\n}\n```\n\n### 3. 모델 목록 조회\n```http\nGET /api/llm/models\n```\n\n**응답:**\n```json\n{\n  \"success\": true,\n  \"models\": [\n    {\n      \"id\": \"ollama\",\n      \"name\": \"Ollama Gemma 3.1B\",\n      \"type\": \"local\",\n      \"status\": \"active\",\n      \"health\": {\n        \"status\": \"healthy\",\n        \"responseTime\": 45\n      },\n      \"priority\": 1,\n      \"cost_per_token\": 0,\n      \"korean_optimized\": true\n    }\n  ],\n  \"currentProvider\": \"ollama\"\n}\n```\n\n### 4. 제공자 전환\n```http\nPOST /api/llm/switch\nContent-Type: application/json\n\n{\n  \"providerId\": \"gemini\"\n}\n```\n\n### 5. 시스템 상태 확인\n```http\nGET /api/llm/health\n```\n\n### 6. 성능 벤치마크\n```http\nPOST /api/llm/benchmark\nContent-Type: application/json\n\n{\n  \"testPrompt\": \"한국어로 안녕하세요라고 답해주세요\"\n}\n```\n\n### 7. 한국어 품질 평가\n```http\nPOST /api/llm/korean-quality\nContent-Type: application/json\n\n{\n  \"text\": \"평가할 한국어 텍스트입니다\"\n}\n```\n\n## 🔧 설정 및 설치\n\n### 1. 환경 변수 설정\n`.env` 파일을 생성하고 다음 설정을 추가:\n\n```bash\n# Ollama 설정\nOLLAMA_ENDPOINT=http://localhost:11434\nOLLAMA_MODEL=gemma2:3b\n\n# 외부 LLM API 키\nGEMINI_API_KEY=your_gemini_api_key\nOPENAI_API_KEY=your_openai_api_key\nCLAUDE_API_KEY=your_claude_api_key\n\n# 기본 설정\nDEFAULT_LLM_PROVIDER=ollama\nPREFER_LOCAL_LLM=true\nDEFAULT_FORMALITY=formal\n```\n\n### 2. Ollama 설치 및 Gemma 3.1B 모델 다운로드\n\n```bash\n# Ollama 설치 (Linux/macOS)\ncurl -fsSL https://ollama.ai/install.sh | sh\n\n# Gemma 3.1B 모델 다운로드\nollama pull gemma2:3b\n\n# Ollama 서비스 시작\nollama serve\n```\n\n### 3. 의존성 설치\n\n```bash\nnpm install\n# 또는\nyarn install\n```\n\n### 4. 시스템 시작\n\n```bash\nnpm start\n# 또는 개발 모드\nnpm run dev\n```\n\n## 🎯 사용 예제\n\n### 1. JavaScript 클라이언트 예제\n\n```javascript\n// 일반 채팅\nconst response = await fetch('/api/llm/chat', {\n  method: 'POST',\n  headers: {\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    messages: [\n      {\n        role: 'user',\n        content: '서버 성능 문제를 분석해주세요'\n      }\n    ],\n    taskType: 'analysis',\n    options: {\n      temperature: 0.3,\n      formality: 'formal'\n    }\n  })\n});\n\nconst data = await response.json();\nconsole.log(data.data.message.content);\n```\n\n### 2. 스트리밍 채팅 예제\n\n```javascript\n// 스트리밍 채팅\nconst eventSource = new EventSource('/api/llm/chat');\n\neventSource.onmessage = function(event) {\n  const data = JSON.parse(event.data);\n  \n  if (data.content) {\n    document.getElementById('response').innerHTML += data.content;\n  }\n  \n  if (data.done) {\n    eventSource.close();\n    console.log('스트리밍 완료');\n  }\n};\n\neventSource.onerror = function(event) {\n  console.error('스트리밍 오류:', event);\n  eventSource.close();\n};\n```\n\n### 3. 시스템 분석 예제\n\n```javascript\n// 성능 데이터 분석\nconst analysisResponse = await fetch('/api/llm/analyze', {\n  method: 'POST',\n  headers: {\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    data: {\n      metrics: {\n        cpu: [85, 90, 95, 88],\n        memory: [70, 75, 80, 78],\n        disk_io: [120, 150, 180, 160]\n      },\n      errors: [\n        {\n          timestamp: '2024-01-15T10:30:00Z',\n          level: 'ERROR',\n          message: 'Database connection timeout'\n        }\n      ]\n    },\n    analysisType: 'performance',\n    language: 'korean'\n  })\n});\n\nconst analysis = await analysisResponse.json();\nconsole.log(analysis.analysis);\n```\n\n## 🔄 작업 유형 (Task Types)\n\n### 1. korean-chat\n- **용도**: 일반적인 한국어 대화\n- **최적화**: 자연스러운 한국어 표현\n- **시스템 프롬프트**: 한국어 대화 전문가\n\n### 2. code-generation\n- **용도**: 코드 생성 및 프로그래밍 도움\n- **최적화**: 한국어 주석 포함 코드\n- **시스템 프롬프트**: 프로그래밍 전문가\n\n### 3. analysis\n- **용도**: 데이터 분석 및 인사이트 제공\n- **최적화**: 구조화된 분석 결과\n- **시스템 프롬프트**: 데이터 분석 전문가\n\n### 4. translation\n- **용도**: 번역 작업\n- **최적화**: 자연스러운 번역\n- **시스템 프롬프트**: 전문 번역가\n\n### 5. tech-support\n- **용도**: 기술 지원 및 문제 해결\n- **최적화**: 단계별 해결 방안\n- **시스템 프롬프트**: 기술 지원 전문가\n\n## 📊 성능 최적화\n\n### 1. 모델 선택 전략\n- **로컬 우선**: Ollama Gemma 3.1B 우선 사용\n- **작업별 최적화**: 작업 유형에 따른 최적 모델 선택\n- **비용 고려**: 토큰당 비용 기반 선택\n- **성능 모니터링**: 응답 시간 및 품질 기반 선택\n\n### 2. 캐싱 전략\n- **응답 캐싱**: 빈번한 쿼리 결과 캐싱\n- **모델 상태 캐싱**: 모델 상태 정보 캐싱\n- **한국어 전처리 캐싱**: 전처리 결과 캐싱\n\n### 3. 로드 밸런싱\n- **요청 분산**: 여러 제공자 간 요청 분산\n- **헬스 체크**: 실시간 제공자 상태 모니터링\n- **자동 복구**: 실패한 제공자 자동 복구\n\n## 🛡️ 보안 고려사항\n\n### 1. API 키 관리\n- **환경 변수**: API 키를 환경 변수로 관리\n- **암호화**: 저장된 키 암호화\n- **접근 제어**: API 키 접근 권한 제한\n\n### 2. 요청 제한\n- **Rate Limiting**: 요청 빈도 제한\n- **토큰 제한**: 최대 토큰 수 제한\n- **사용자 인증**: 사용자별 접근 제어\n\n### 3. 데이터 보안\n- **전송 암호화**: HTTPS 사용\n- **로그 관리**: 민감한 정보 로그 제외\n- **데이터 최소화**: 필요한 데이터만 전송\n\n## 🔍 모니터링 및 로깅\n\n### 1. 성능 메트릭\n- **응답 시간**: 평균 응답 시간 추적\n- **토큰 사용량**: 토큰 소비 패턴 분석\n- **오류율**: 오류 발생 빈도 모니터링\n- **제공자 상태**: 각 제공자별 상태 추적\n\n### 2. 품질 메트릭\n- **한국어 품질 점수**: 응답 품질 평가\n- **사용자 만족도**: 사용자 피드백 수집\n- **정확도**: 응답 정확도 측정\n\n### 3. 운영 메트릭\n- **시스템 리소스**: CPU, 메모리 사용량\n- **네트워크 사용량**: 대역폭 사용 패턴\n- **디스크 사용량**: 캐시 및 로그 사용량\n\n## 🚨 문제 해결 가이드\n\n### 1. Ollama 연결 문제\n```bash\n# Ollama 서비스 상태 확인\nsudo systemctl status ollama\n\n# Ollama 서비스 재시작\nsudo systemctl restart ollama\n\n# 모델 목록 확인\nollama list\n\n# Gemma 모델 재다운로드\nollama pull gemma2:3b\n```\n\n### 2. 외부 API 연결 문제\n- **API 키 검증**: API 키 유효성 확인\n- **네트워크 연결**: 외부 서비스 접근 가능성 확인\n- **요청 제한**: API 사용량 한도 확인\n\n### 3. 성능 문제\n- **로드 밸런싱**: 요청 분산 확인\n- **캐시 상태**: 캐시 적중률 확인\n- **리소스 사용량**: 시스템 리소스 모니터링\n\n## 📚 추가 자료\n\n- [Ollama 공식 문서](https://ollama.ai/docs)\n- [Google Gemini API 문서](https://developers.generativeai.google/docs)\n- [OpenAI API 문서](https://platform.openai.com/docs)\n- [Anthropic Claude API 문서](https://docs.anthropic.com/)\n- [AIRIS-MON 시스템 문서](./README.md)\n\n## 🔄 업데이트 및 유지보수\n\n### 1. 모델 업데이트\n```bash\n# Gemma 모델 업데이트\nollama pull gemma2:3b\n\n# 새 모델 버전 확인\nollama list\n```\n\n### 2. 시스템 업데이트\n```bash\n# 의존성 업데이트\nnpm update\n\n# 시스템 재시작\nnpm restart\n```\n\n### 3. 설정 백업\n```bash\n# 설정 파일 백업\ncp .env .env.backup\ncp -r src/llm/ backup/llm/\n```\n\n---\n\n**버전**: 1.0.0  \n**최종 업데이트**: 2024-01-15  \n**작성자**: AIRIS-MON Development Team\n