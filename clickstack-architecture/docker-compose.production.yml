# AIRIS-MON ClickStack Architecture - Production Docker Compose
# Korean-optimized observability platform with full ClickStack integration

version: '3.8'

services:
  # ClickHouse Database (Primary Data Store)
  clickhouse:
    image: yandex/clickhouse-server:latest
    container_name: airis-mon-clickhouse
    hostname: clickhouse
    ports:
      - "8123:8123"  # HTTP interface
      - "9000:9000"  # Native interface
    environment:
      CLICKHOUSE_DB: airis_mon
      CLICKHOUSE_USER: admin
      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: 1
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - ./services/clickhouse/config:/etc/clickhouse-server/config.d
      - ./services/clickhouse/migrations:/docker-entrypoint-initdb.d
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
        reservations:
          memory: 2G
          cpus: '1.0'
    networks:
      - airis-mon-network
    healthcheck:
      test: ["CMD", "clickhouse-client", "--query", "SELECT 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  # Zookeeper (Kafka Dependency)
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    container_name: airis-mon-zookeeper
    hostname: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_SYNC_LIMIT: 2
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_MAX_CLIENT_CNXNS: 60
      ZOOKEEPER_AUTOPURGE_SNAP_RETAIN_COUNT: 3
      ZOOKEEPER_AUTOPURGE_PURGE_INTERVAL: 24
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
      - zookeeper_logs:/var/lib/zookeeper/log
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
    networks:
      - airis-mon-network
    healthcheck:
      test: ["CMD", "echo", "ruok", "|", "nc", "localhost", "2181"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # Kafka (Message Streaming)
  kafka:
    image: confluentinc/cp-kafka:latest
    container_name: airis-mon-kafka
    hostname: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_NUM_PARTITIONS: 3
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_DELETE_TOPIC_ENABLE: 'true'
      KAFKA_LOG_RETENTION_HOURS: 168  # 7 days
      KAFKA_LOG_SEGMENT_BYTES: 1073741824  # 1GB
      KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS: 300000
      KAFKA_MESSAGE_MAX_BYTES: 10485760  # 10MB
      KAFKA_COMPRESSION_TYPE: 'snappy'
    volumes:
      - kafka_data:/var/lib/kafka/data
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'
    networks:
      - airis-mon-network
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped

  # Redis (Caching and Session Management)
  redis:
    image: redis:7-alpine
    container_name: airis-mon-redis
    hostname: redis
    ports:
      - "6379:6379"
    command: >
      redis-server 
      --appendonly yes 
      --appendfsync everysec 
      --maxmemory 1gb 
      --maxmemory-policy allkeys-lru
      --tcp-keepalive 60
      --timeout 300
    volumes:
      - redis_data:/data
      - ./services/redis/config/redis.conf:/etc/redis/redis.conf:ro
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 512M
          cpus: '0.25'
    networks:
      - airis-mon-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # API Gateway (Main Entry Point)
  api-gateway:
    build:
      context: ./services/api-gateway
      dockerfile: Dockerfile
      target: production
    container_name: airis-mon-api-gateway
    hostname: api-gateway
    depends_on:
      - clickhouse
      - kafka
      - redis
    ports:
      - "3002:3002"
      - "8080:8080"  # Health check port
    environment:
      NODE_ENV: production
      PORT: 3002
      HEALTH_PORT: 8080
      CLICKHOUSE_HOST: clickhouse
      CLICKHOUSE_PORT: 8123
      CLICKHOUSE_USER: admin
      CLICKHOUSE_DATABASE: airis_mon
      KAFKA_BROKERS: kafka:29092
      REDIS_HOST: redis
      REDIS_PORT: 6379
      LOG_LEVEL: info
      TIMEZONE: Asia/Seoul
      KOREAN_BUSINESS_HOURS_START: 9
      KOREAN_BUSINESS_HOURS_END: 18
    volumes:
      - ./logs/api-gateway:/app/logs
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.5'
      replicas: 2
    networks:
      - airis-mon-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped

  # Data Ingestion Service
  data-ingestion:
    build:
      context: ./services/data-ingestion
      dockerfile: Dockerfile
      target: production
    container_name: airis-mon-data-ingestion
    hostname: data-ingestion
    depends_on:
      - clickhouse
      - kafka
      - redis
    environment:
      NODE_ENV: production
      CLICKHOUSE_HOST: clickhouse
      CLICKHOUSE_PORT: 8123
      CLICKHOUSE_USER: admin
      CLICKHOUSE_DATABASE: airis_mon
      KAFKA_BROKERS: kafka:29092
      REDIS_HOST: redis
      REDIS_PORT: 6379
      BATCH_SIZE: 1000
      FLUSH_INTERVAL: 5000
      MAX_RETRIES: 3
      LOG_LEVEL: info
      TIMEZONE: Asia/Seoul
    volumes:
      - ./logs/data-ingestion:/app/logs
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.5'
      replicas: 2
    networks:
      - airis-mon-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # Analytics Engine
  analytics-engine:
    build:
      context: ./services/analytics-engine
      dockerfile: Dockerfile
      target: production
    container_name: airis-mon-analytics-engine
    hostname: analytics-engine
    depends_on:
      - clickhouse
      - kafka
      - redis
    environment:
      NODE_ENV: production
      CLICKHOUSE_HOST: clickhouse
      CLICKHOUSE_PORT: 8123
      CLICKHOUSE_USER: admin
      CLICKHOUSE_DATABASE: airis_mon
      KAFKA_BROKERS: kafka:29092
      REDIS_HOST: redis
      REDIS_PORT: 6379
      ANOMALY_THRESHOLD: 2.5
      TREND_ANALYSIS_WINDOW: 24
      KOREAN_BUSINESS_START: 9
      KOREAN_BUSINESS_END: 18
      LOG_LEVEL: info
      TIMEZONE: Asia/Seoul
    volumes:
      - ./logs/analytics-engine:/app/logs
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.5'
        reservations:
          memory: 1G
          cpus: '0.75'
    networks:
      - airis-mon-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # Alert Manager
  alert-manager:
    build:
      context: ./services/alert-manager
      dockerfile: Dockerfile
      target: production
    container_name: airis-mon-alert-manager
    hostname: alert-manager
    depends_on:
      - clickhouse
      - kafka
      - redis
    environment:
      NODE_ENV: production
      CLICKHOUSE_HOST: clickhouse
      CLICKHOUSE_PORT: 8123
      CLICKHOUSE_USER: admin
      CLICKHOUSE_DATABASE: airis_mon
      KAFKA_BROKERS: kafka:29092
      REDIS_HOST: redis
      REDIS_PORT: 6379
      KOREAN_BUSINESS_START: 9
      KOREAN_BUSINESS_END: 18
      SLACK_WEBHOOK_URL: ${SLACK_WEBHOOK_URL:-}
      EMAIL_SMTP_HOST: ${EMAIL_SMTP_HOST:-}
      EMAIL_SMTP_USER: ${EMAIL_SMTP_USER:-}
      EMAIL_SMTP_PASS: ${EMAIL_SMTP_PASS:-}
      SMS_API_KEY: ${SMS_API_KEY:-}
      LOG_LEVEL: info
      TIMEZONE: Asia/Seoul
    volumes:
      - ./logs/alert-manager:/app/logs
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.5'
    networks:
      - airis-mon-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # Korean HyperDX Dashboard
  korean-dashboard:
    build:
      context: ./ui/korean-hyperdx-dashboard
      dockerfile: Dockerfile
      target: production
    container_name: airis-mon-korean-dashboard
    hostname: korean-dashboard
    depends_on:
      - api-gateway
    ports:
      - "3000:80"
      - "3001:3001"  # Development server (if needed)
    environment:
      NODE_ENV: production
      REACT_APP_API_BASE_URL: http://api-gateway:3002
      REACT_APP_WS_URL: ws://api-gateway:3002
      REACT_APP_KOREAN_LOCALE: ko-KR
      REACT_APP_TIMEZONE: Asia/Seoul
      REACT_APP_BUSINESS_HOURS_START: 9
      REACT_APP_BUSINESS_HOURS_END: 18
      REACT_APP_VERSION: ${APP_VERSION:-1.0.0}
    volumes:
      - ./logs/dashboard:/var/log/nginx
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'
    networks:
      - airis-mon-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # NGINX Reverse Proxy (Production Load Balancer)
  nginx:
    image: nginx:alpine
    container_name: airis-mon-nginx
    hostname: nginx
    depends_on:
      - api-gateway
      - korean-dashboard
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/conf.d:/etc/nginx/conf.d:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - ./logs/nginx:/var/log/nginx
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
    networks:
      - airis-mon-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # Prometheus (Metrics Collection)
  prometheus:
    image: prom/prometheus:latest
    container_name: airis-mon-prometheus
    hostname: prometheus
    ports:
      - "9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=7d'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/prometheus/rules:/etc/prometheus/rules:ro
      - prometheus_data:/prometheus
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'
    networks:
      - airis-mon-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # Grafana (Visualization - Backup to Korean Dashboard)
  grafana:
    image: grafana/grafana:latest
    container_name: airis-mon-grafana
    hostname: grafana
    depends_on:
      - prometheus
    ports:
      - "3333:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD:-admin123}
      GF_INSTALL_PLUGINS: grafana-clickhouse-datasource
      GF_DEFAULT_TIMEZONE: Asia/Seoul
      GF_DATE_FORMATS_DEFAULT_TIMEZONE: Asia/Seoul
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources:ro
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
    networks:
      - airis-mon-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # Jaeger (Distributed Tracing)
  jaeger:
    image: jaegertracing/all-in-one:latest
    container_name: airis-mon-jaeger
    hostname: jaeger
    ports:
      - "16686:16686"  # Jaeger UI
      - "14268:14268"  # HTTP collector
    environment:
      COLLECTOR_ZIPKIN_HOST_PORT: 9411
      SPAN_STORAGE_TYPE: memory
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
    networks:
      - airis-mon-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:16686/"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

# Named Volumes
volumes:
  clickhouse_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./data/clickhouse
  zookeeper_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./data/zookeeper/data
  zookeeper_logs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./data/zookeeper/logs
  kafka_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./data/kafka
  redis_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./data/redis
  prometheus_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./data/prometheus
  grafana_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./data/grafana

# Networks
networks:
  airis-mon-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
          gateway: 172.20.0.1
    driver_opts:
      com.docker.network.bridge.name: airis-mon-bridge
      com.docker.network.bridge.enable_ip_masquerade: 'true'
      com.docker.network.bridge.enable_icc: 'true'
      com.docker.network.bridge.host_binding_ipv4: '0.0.0.0'

# Production Configuration
x-logging: &default-logging
  driver: "json-file"
  options:
    max-size: "100m"
    max-file: "3"

x-restart-policy: &default-restart-policy
  restart_policy:
    condition: on-failure
    delay: 30s
    max_attempts: 3
    window: 120s

# Health Check Configuration
x-healthcheck: &default-healthcheck
  interval: 30s
  timeout: 10s
  retries: 3
  start_period: 60s