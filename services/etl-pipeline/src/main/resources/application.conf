# AIRIS EPM ETL Pipeline Configuration

pipeline {
  name = "airis-epm-etl-pipeline"
  version = "1.0.0"
  environment = ${?ENVIRONMENT}
  
  # Parallelism and performance
  parallelism = 4
  maxParallelism = 128
  checkpointInterval = 5000  # 5 seconds
  checkpointTimeout = 60000  # 1 minute
  
  # State backend configuration
  stateBackend = "rocksdb"
  rocksdb {
    localDir = "/tmp/flink-rocksdb"
    remoteCheckpointDir = "hdfs://namenode:9000/flink/checkpoints"
    incrementalCheckpoints = true
  }
}

kafka {
  bootstrap.servers = "localhost:9092"
  bootstrap.servers = ${?KAFKA_BOOTSTRAP_SERVERS}
  
  # Consumer configuration
  consumer {
    group.id = "airis-etl-pipeline"
    auto.offset.reset = "latest"
    enable.auto.commit = false
    max.poll.records = 500
    fetch.max.wait.ms = 500
    
    # Performance tuning
    receive.buffer.bytes = 65536
    send.buffer.bytes = 131072
  }
  
  # Producer configuration  
  producer {
    acks = "all"
    retries = 2147483647
    max.in.flight.requests.per.connection = 5
    enable.idempotence = true
    compression.type = "lz4"
    
    # Performance tuning
    batch.size = 16384
    linger.ms = 5
    buffer.memory = 33554432
  }
  
  # Topics configuration
  topics {
    input {
      metrics = "airis.metrics.raw"
      logs = "airis.logs.raw"  
      traces = "airis.traces.raw"
      events = "airis.events.raw"
      alerts = "airis.alerts.raw"
    }
    
    output {
      processedMetrics = "airis.metrics.processed"
      processedLogs = "airis.logs.processed"
      processedTraces = "airis.traces.processed"
      aggregatedMetrics = "airis.metrics.aggregated"
      anomalies = "airis.anomalies.detected"
    }
    
    deadletter = "airis.deadletter"
  }
}

clickhouse {
  url = "jdbc:clickhouse://localhost:8123/airis_epm"
  url = ${?CLICKHOUSE_URL}
  username = "default"
  username = ${?CLICKHOUSE_USERNAME}
  password = ""
  password = ${?CLICKHOUSE_PASSWORD}
  
  # Connection pool
  maxConnections = 10
  connectionTimeout = 30000
  socketTimeout = 60000
  
  # Batch insertion
  batchSize = 1000
  flushInterval = 5000  # 5 seconds
  maxRetries = 3
  
  # Tables
  tables {
    metrics = "metrics_real_time"
    logs = "logs_real_time"
    traces = "traces_real_time"
    events = "events_real_time" 
    aggregated_metrics = "metrics_aggregated"
  }
}

elasticsearch {
  hosts = ["http://localhost:9200"]
  hosts = [${?ELASTICSEARCH_HOSTS}]
  
  # Authentication
  username = ${?ELASTICSEARCH_USERNAME}
  password = ${?ELASTICSEARCH_PASSWORD}
  
  # Connection settings
  connectionTimeout = 10000
  socketTimeout = 60000
  maxRetryTimeout = 60000
  
  # Bulk configuration
  bulkActions = 1000
  bulkSizeMB = 5
  flushInterval = 5000
  
  # Indices
  indices {
    logs = "airis-logs"
    traces = "airis-traces" 
    events = "airis-events"
    anomalies = "airis-anomalies"
  }
}

redis {
  host = "localhost"
  host = ${?REDIS_HOST}
  port = 6379
  port = ${?REDIS_PORT}
  database = 0
  password = ${?REDIS_PASSWORD}
  
  # Connection pool
  maxConnections = 10
  timeout = 5000
  
  # TTL for cached data (seconds)
  ttl {
    metrics = 300      # 5 minutes
    enrichment = 1800  # 30 minutes
    sessions = 3600    # 1 hour
  }
}

# Data quality and validation
quality {
  # Schema validation
  validateSchemas = true
  schemaRegistry = "http://localhost:8081"
  
  # Data completeness checks
  requiredFields = ["timestamp", "source", "type"]
  
  # Duplicate detection
  deduplicationWindow = 300  # 5 minutes
  deduplicationKeyFields = ["id", "timestamp", "source"]
  
  # Anomaly detection
  anomalyDetection {
    enabled = true
    algorithm = "isolation_forest"
    threshold = 0.1
    trainingWindow = 3600  # 1 hour
  }
}

# Windowing configuration
windowing {
  tumbling {
    defaultSize = "1 minute"
    sizes = {
      metrics = "30 seconds"
      logs = "5 minutes"
      traces = "2 minutes"
    }
  }
  
  sliding {
    defaultSize = "5 minutes"
    defaultSlide = "1 minute"
  }
  
  session {
    timeout = "30 minutes"
    maxDuration = "2 hours"
  }
}

# Alerting configuration
alerting {
  enabled = true
  
  # Alert channels
  channels {
    kafka {
      topic = "airis.alerts.notifications"
      enabled = true
    }
    
    webhook {
      url = ${?ALERT_WEBHOOK_URL}
      enabled = false
    }
    
    email {
      smtp.host = ${?SMTP_HOST}
      smtp.port = 587
      enabled = false
    }
  }
  
  # Alert rules
  rules {
    highErrorRate {
      condition = "error_rate > 0.05"
      window = "5 minutes"
      severity = "warning"
    }
    
    highLatency {
      condition = "p99_latency > 5000"
      window = "2 minutes" 
      severity = "critical"
    }
    
    lowThroughput {
      condition = "throughput < 100"
      window = "10 minutes"
      severity = "warning"
    }
  }
}

# Monitoring and metrics
monitoring {
  enabled = true
  
  # Prometheus metrics
  prometheus {
    enabled = true
    port = 9090
    path = "/metrics"
  }
  
  # JMX metrics
  jmx {
    enabled = true
    port = 9999
  }
  
  # Health checks
  health {
    enabled = true
    port = 8080
    path = "/health"
    interval = 30  # seconds
  }
  
  # Custom metrics
  customMetrics {
    recordsProcessed = true
    processingLatency = true
    errorRate = true
    backpressure = true
  }
}

# Logging configuration
logging {
  level = "INFO"
  level = ${?LOG_LEVEL}
  
  # Appenders
  console {
    enabled = true
    pattern = "%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n"
  }
  
  file {
    enabled = true
    path = "logs/etl-pipeline.log"
    maxSize = "100MB"
    maxHistory = 30
  }
  
  # Structured logging
  json {
    enabled = true
    includeStackTrace = true
    includeMdc = true
  }
}

# Security configuration
security {
  ssl {
    enabled = false
    keystore = ${?SSL_KEYSTORE_PATH}
    keystorePassword = ${?SSL_KEYSTORE_PASSWORD}
    truststore = ${?SSL_TRUSTSTORE_PATH}
    truststorePassword = ${?SSL_TRUSTSTORE_PASSWORD}
  }
  
  authentication {
    enabled = false
    type = "kerberos"
    principal = ${?KERBEROS_PRINCIPAL}
    keytab = ${?KERBEROS_KEYTAB}
  }
}

# Development and testing
development {
  # Mock data generation
  mockData {
    enabled = false
    recordsPerSecond = 1000
    duration = "1 hour"
  }
  
  # Local debugging
  debug {
    enableCheckpointing = false
    webUIPort = 8081
    localExecution = true
  }
}

# Production optimizations
production {
  # Resource allocation
  resources {
    taskManagerMemory = "2048m"
    jobManagerMemory = "1024m"
    networkBuffers = "2048"
  }
  
  # Fault tolerance
  faultTolerance {
    restartStrategy = "fixed-delay"
    restartAttempts = 3
    restartDelay = "10s"
  }
  
  # Performance tuning  
  performance {
    bufferTimeout = 100
    networkBufferSize = "32kb"
    managedMemoryFraction = 0.4
  }
}